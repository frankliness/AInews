"""
Phoenix V2 æ–°é—»æµæ°´çº¿ DAG
ä½¿ç”¨äº‹ä»¶ä¼˜å…ˆç­–ç•¥çš„æ–°ä¸€ä»£æ–°é—»åˆ†æç³»ç»Ÿ
"""
from datetime import datetime, timedelta
import logging
import sys
import os
from typing import List, Dict, Any

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/opt/airflow')

from scraper.newsapi_client import NewsAPIClient

# é…ç½®æ—¥å¿—
log = logging.getLogger(__name__)

# DAGé…ç½®
default_args = {
    'owner': 'phoenix',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'phoenix_news_pipeline',
    default_args=default_args,
    description='Phoenix V2 äº‹ä»¶ä¼˜å…ˆæ–°é—»æµæ°´çº¿',
    schedule_interval='0 * * * *',  # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
    catchup=False,
    tags=['phoenix', 'v2', 'news'],
)

def fetch_and_ingest_news(**context):
    """
    è·å–å¹¶å…¥åº“æ–°é—»æ•°æ®çš„ä¸»å‡½æ•°
    """
    log.info("ğŸš€ Phoenix V2 å¼€å§‹æ‰§è¡Œæ–°é—»æŠ“å–ä»»åŠ¡")
    
    try:
        # a. ä»Airflow Variableä¸­è·å– NEWSAPI_AI_KEY
        api_key = Variable.get("NEWSAPI_AI_KEY")
        log.info("âœ… æˆåŠŸè·å–APIå¯†é’¥")
        
        # b. å®ä¾‹åŒ– NewsAPIClient
        client = NewsAPIClient(api_key)
        
        # c. è°ƒç”¨ fetch_trending_events_urisï¼Œè·å–çƒ­é—¨äº‹ä»¶URI
        category_uris = [
            "dmoz/Business",
            "dmoz/Technology", 
            "dmoz/Politics",
            "dmoz/Health",
            "dmoz/Science"
        ]
        date_start = datetime.now() - timedelta(hours=24)
        
        event_uris = client.fetch_trending_events_uris(category_uris, date_start)
        
        if not event_uris:
            log.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•äº‹ä»¶URI")
            return
        
        # æµ‹è¯•é˜¶æ®µï¼šåªå¤„ç†å‰5ä¸ªäº‹ä»¶
        event_uris = event_uris[:5]
        log.info(f"ğŸ“Š è·å–åˆ° {len(event_uris)} ä¸ªäº‹ä»¶URI (æµ‹è¯•æ¨¡å¼ï¼Œä»…å¤„ç†å‰5ä¸ª)")
        
        # d. åˆ›å»ºç©ºåˆ—è¡¨ all_articles
        all_articles = []
        
        # e. å¾ªç¯éå†URIï¼Œè·å–æ–‡ç« æ•°æ®
        for event_uri in event_uris:
            articles = client.fetch_rich_articles_for_event(event_uri)
            all_articles.extend(articles)
            
            # æ·»åŠ å°å»¶è¿Ÿé¿å…APIé™åˆ¶
            import time
            time.sleep(0.5)
        
        log.info(f"ğŸ“° æ€»å…±è·å–åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
        
        if not all_articles:
            log.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®")
            return
        
        # f. ä½¿ç”¨PostgresHookæ‰¹é‡æ’å…¥æ•°æ®
        pg_hook = PostgresHook(postgres_conn_id='postgres_phoenix_shadow')
        
        # å‡†å¤‡æ’å…¥æ•°æ®
        insert_data = []
        for article in all_articles:
            row = (
                article.get("title", ""),
                article.get("body", ""),
                article.get("url", ""),
                article.get("published_at"),
                article.get("sentiment", 0.0),
                article.get("relevance", 1.0),
                article.get("weight", 1.0),
                article.get("event_uri", ""),
                article.get("source_name", ""),
                article.get("source_importance", 1),
                article.get("total_articles_24h", 0),
                article.get("embedding"),
                datetime.now(),  # collected_at
                "phoenix_v2"  # source
            )
            insert_data.append(row)
        
        # æ‰§è¡Œæ‰¹é‡æ’å…¥
        columns = [
            "title", "body", "url", "published_at", "sentiment", 
            "relevance", "weight", "event_uri", "source_name", 
            "source_importance", "total_articles_24h", "embedding",
            "collected_at", "source"
        ]
        
        pg_hook.insert_rows(
            table="raw_events",
            rows=insert_data,
            target_fields=columns
        )
        
        log.info(f"âœ… æˆåŠŸæ’å…¥ {len(insert_data)} æ¡è®°å½•åˆ° phoenix_shadow.raw_events")
        
        # è®¾ç½®ä»»åŠ¡æˆåŠŸçŠ¶æ€
        context['task_instance'].xcom_push(key='articles_count', value=len(all_articles))
        context['task_instance'].xcom_push(key='events_count', value=len(event_uris))
        
    except Exception as e:
        log.error(f"âŒ Phoenix V2 ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        raise

# åˆ›å»ºä»»åŠ¡
fetch_and_ingest_task = PythonOperator(
    task_id='fetch_and_ingest_task',
    python_callable=fetch_and_ingest_news,
    provide_context=True,
    dag=dag,
)

# ä»»åŠ¡ä¾èµ–å…³ç³»ï¼ˆç›®å‰åªæœ‰ä¸€ä¸ªä»»åŠ¡ï¼‰
fetch_and_ingest_task 