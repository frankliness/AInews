"""
é€šç”¨ EventRegistry v1 å°è£…ï¼ˆå…è´¹ç‰ˆæœ¬ï¼‰ï¼š
    https://eventregistry.org/documentation
"""
from __future__ import annotations
import os, logging, time, json
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from .base import ScrapedItem, BaseScraper
from .utils import is_within_24h, log_news_record, check_duplicate_url, simple_sentiment_analysis

log = logging.getLogger(__name__)
_ENDPOINT = "https://eventregistry.org/api/v1/article/getArticles"

def _get_api_key():
    """è·å– API keyï¼Œå¦‚æœæœªè®¾ç½®åˆ™æŠ›å‡ºå¼‚å¸¸"""
    apikey = os.getenv("EVENTREGISTRY_APIKEY")
    if not apikey:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ EVENTREGISTRY_APIKEY")
    return apikey

def _create_session():
    """åˆ›å»ºå¸¦æœ‰é‡è¯•æœºåˆ¶çš„ä¼šè¯"""
    session = requests.Session()
    
    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=3,  # æœ€å¤šé‡è¯•3æ¬¡
        backoff_factor=1,  # é‡è¯•é—´éš”ï¼š1, 2, 4ç§’
        status_forcelist=[429, 500, 502, 503, 504],  # è¿™äº›çŠ¶æ€ç ä¼šé‡è¯•
        allowed_methods=["POST", "GET"]  # å…è®¸é‡è¯•çš„HTTPæ–¹æ³•
    )
    
    # é…ç½®é€‚é…å™¨
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

def _event_date_time(article: Dict) -> datetime:
    # EventRegistry å­—æ®µ dateTimePub å½¢å¦‚ '2025-07-09T07:16:42Z'
    ts = article.get("dateTimePub") or article.get("dateTime")
    if not ts:
        # å¦‚æœæ²¡æœ‰æ—¶é—´å­—æ®µï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        return datetime.now(timezone.utc)
    return datetime.fromisoformat(ts.replace("Z", "+00:00")).astimezone(timezone.utc)

def _extract_event_info(article: Dict) -> Dict[str, Any]:
    """æå–EventRegistryäº‹ä»¶ä¿¡æ¯"""
    # æå–event_id
    event_id = article.get("eventUri") or article.get("eventId")
    
    # æå–embeddingï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    embedding = None
    if "embedding" in article:
        embedding = json.dumps(article["embedding"])
    
    # æå–äº‹ä»¶æ–‡ç« æ€»æ•°
    total_articles_24h = article.get("totalArticleCount", 0)
    
    # æå–æºé‡è¦æ€§
    source_importance = article.get("sourceImportance", 1)
    
    # æå–æƒé‡
    weight = article.get("wgt", 1)
    
    return {
        "event_id": event_id,
        "embedding": embedding,
        "total_articles_24h": total_articles_24h,
        "source_importance": source_importance,
        "weight": weight
    }

def fetch_articles(query: Dict) -> List[Dict]:
    """è·å–æ–‡ç« åˆ—è¡¨ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""
    payload = {**query, "apiKey": _get_api_key()}
    
    # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
    timeout = (30, 60)  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
    
    try:
        session = _create_session()
        resp = session.post(_ENDPOINT, json=payload, timeout=timeout)
        resp.raise_for_status()
        results = resp.json().get("articles", {}).get("results", [])
        
        # æ£€æŸ¥APIè¿”å›çš„æ•°æ®é‡ï¼Œä½†ä¸å¼ºåˆ¶é™åˆ¶
        requested_count = query.get("count", 50)
        if len(results) > requested_count:
            log.warning(f"APIè¿”å› {len(results)} æ¡æ•°æ®ï¼Œè¶…è¿‡è¯·æ±‚çš„ {requested_count} æ¡")
        
        return results
        
    except requests.exceptions.Timeout as e:
        log.error(f"EventRegistry API è¯·æ±‚è¶…æ—¶: {e}")
        raise
    except requests.exceptions.ConnectionError as e:
        log.error(f"EventRegistry API è¿æ¥é”™è¯¯: {e}")
        raise
    except requests.exceptions.RequestException as e:
        log.error(f"EventRegistry API è¯·æ±‚å¤±è´¥: {e}")
        raise
    except Exception as e:
        log.error(f"EventRegistry API æœªçŸ¥é”™è¯¯: {e}")
        raise

class BaseNewsAPIScraper(BaseScraper):
    """å­ç±»åªè¦ç»™å‡º self.queryï¼ˆkeyword / sourceUri / langâ€¦ï¼‰"""

    query: Dict = {}
    source: str = "newsapi"

    def fetch(self):
        try:
            return fetch_articles(self.query)
        except Exception as e:
            log.error(f"æŠ“å–å¤±è´¥ {self.source}: {e}")
            return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

    def parse(self, raw, db_connection=None):
        collected_at = datetime.now(ZoneInfo("Asia/Shanghai"))
        
        # ç»Ÿè®¡å˜é‡
        total_processed = 0
        sentiment_filtered = 0
        
        for art in raw:
            # è§£æå‘å¸ƒæ—¶é—´
            published_at = _event_date_time(art)
            
            # æ—¶é—´è¿‡æ»¤ï¼šåªå¤„ç†è¿‡å»24å°æ—¶å†…çš„æ•°æ®
            if not is_within_24h(published_at):
                log.debug(f"è·³è¿‡è¿‡æœŸæ•°æ®: {art.get('title', 'Unknown')[:50]}...")
                continue
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            title = art.get("title") or ""
            body = art.get("body") or ""
            url = art.get("url")
            sentiment = art.get("sentiment")
            if sentiment is None:
                sentiment = 0.0
            
            # æå–EventRegistryäº‹ä»¶ä¿¡æ¯
            event_info = _extract_event_info(art)
            
            # å¦‚æœæ²¡æœ‰æƒ…æ„Ÿåˆ†æï¼Œè¿›è¡Œç®€å•åˆ†æ
            if sentiment == 0.0:
                sentiment = simple_sentiment_analysis(title + " " + body)
            
            # ç»Ÿè®¡å¤„ç†æ•°é‡
            total_processed += 1
            
            # æƒ…æ„Ÿè¿‡æ»¤ï¼šåªä¿ç•™æƒ…æ„Ÿå€¼å¤§äº0.3æˆ–å°äº-0.3çš„æ–°é—»
            if not (sentiment > 0.3 or sentiment < -0.3):
                sentiment_filtered += 1
                log.debug(f"è·³è¿‡æƒ…æ„Ÿå€¼ä¸­æ€§çš„æ–°é—»: {art.get('title', 'Unknown')[:50]}... (sentiment: {sentiment:.3f})")
                continue
            
            # å»é‡æ£€æŸ¥
            if db_connection and url:
                if check_duplicate_url(url, db_connection):
                    log.debug(f"è·³è¿‡é‡å¤URL: {url}")
                    continue
            
            # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆä¸è§¦å‘æ±‡æ€»ï¼‰
            try:
                log_news_record(
                    source=self.source,
                    collected_at=collected_at,
                    published_at=published_at,
                    title=title,
                    body=body,
                    url=url,
                    sentiment=sentiment,
                    weight=event_info["weight"],
                    relevance=art.get("relevance", 1.0),
                    event_id=event_info["event_id"],
                    embedding=event_info["embedding"],
                    total_articles_24h=event_info["total_articles_24h"],
                    source_importance=event_info["source_importance"]
                )
            except Exception as e:
                log.error(f"è®°å½•æ—¥å¿—å¤±è´¥: {e}")
            
            yield ScrapedItem(
                source        = self.source,
                title         = title,
                body          = body,
                published_at  = published_at,
                url           = url,
                likes         = 0,
                retweets      = 0,
                event_id      = event_info["event_id"],
                embedding     = event_info["embedding"],
                total_articles_24h = event_info["total_articles_24h"],
                source_importance = event_info["source_importance"],
                weight        = event_info["weight"],
                sentiment     = sentiment
            )
        
        # è¾“å‡ºæƒ…æ„Ÿè¿‡æ»¤ç»Ÿè®¡
        if total_processed > 0:
            filtered_ratio = sentiment_filtered / total_processed * 100
            log.info(f"ğŸ“Š {self.source} æƒ…æ„Ÿè¿‡æ»¤ç»Ÿè®¡: å¤„ç† {total_processed} æ¡ï¼Œè¿‡æ»¤ {sentiment_filtered} æ¡ (è¿‡æ»¤ç‡: {filtered_ratio:.1f}%)")
