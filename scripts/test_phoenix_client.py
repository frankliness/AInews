#!/usr/bin/env python3
"""
æµ‹è¯• Phoenix NewsAPIClient åŠŸèƒ½
éªŒè¯ä¿¡æºè¿‡æ»¤ã€æ•°é‡æ§åˆ¶å’ŒAPIæ¶ˆè€—ç›‘æ§
"""

import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from config.settings import TRUSTED_SOURCES  # å·²è¿ç§»è‡³ Airflow Variable: TRUSTED_SOURCES_WHITELIST
# from config.settings import MAX_EVENTS_TO_FETCH, ARTICLES_PER_EVENT  # å·²è¿ç§»è‡³ Airflow Variables
from scraper.newsapi_client import NewsApiClient

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger(__name__)

def test_phoenix_client():
    """æµ‹è¯• Phoenix NewsAPIClient çš„æ ¸å¿ƒåŠŸèƒ½"""
    
    # 1. è·å–APIå¯†é’¥
    api_key = os.getenv('EVENTREGISTRY_APIKEY')
    if not api_key:
        log.error("ç¯å¢ƒå˜é‡ EVENTREGISTRY_APIKEY æœªè®¾ç½®")
        return False
    
    log.info("âœ… APIå¯†é’¥å·²è·å–")
    
    # 2. åˆå§‹åŒ–å®¢æˆ·ç«¯
    try:
        client = NewsApiClient(api_key=api_key)
        log.info("âœ… NewsApiClient åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        log.error(f"âŒ NewsApiClient åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•APIä½™é‡æŸ¥è¯¢
    requests_before = client.get_remaining_requests()
    log.info(f"ğŸ“Š APIè¯·æ±‚ä½™é‡: {requests_before}")
    
    # 4. æµ‹è¯•ä¿¡æºURIè½¬æ¢ï¼ˆä½¿ç”¨æµ‹è¯•ç”¨çš„å›ºå®šä¿¡æºåˆ—è¡¨ï¼‰
    test_sources = ["reuters.com", "bbc.com", "nytimes.com", "cnn.com", "apnews.com"]
    log.info(f"ğŸ” æµ‹è¯•ä¿¡æºURIè½¬æ¢ï¼Œä½¿ç”¨æµ‹è¯•ä¿¡æºåˆ—è¡¨: {len(test_sources)} ä¸ªä¿¡æº")
    source_uris = client.get_uris_for_sources(test_sources)
    log.info(f"âœ… æˆåŠŸè½¬æ¢ {len(source_uris)} ä¸ªä¿¡æºURI")
    
    # 5. æµ‹è¯•äº‹ä»¶è·å–ï¼ˆä½¿ç”¨è¾ƒå°çš„é™åˆ¶è¿›è¡Œæµ‹è¯•ï¼‰
    log.info(f"ğŸ“° æµ‹è¯•äº‹ä»¶è·å–ï¼Œé™åˆ¶: 3 ä¸ªäº‹ä»¶ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
    events = client.fetch_trending_events(
        source_names=test_sources,  # ä½¿ç”¨æµ‹è¯•ä¿¡æºåˆ—è¡¨
        max_events=3  # åªè·å–3ä¸ªäº‹ä»¶è¿›è¡Œæµ‹è¯•
    )
    
    if not events:
        log.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•äº‹ä»¶")
        return False
    
    log.info(f"âœ… æˆåŠŸè·å– {len(events)} ä¸ªäº‹ä»¶")
    
    # 6. æµ‹è¯•æ–‡ç« è·å–
    test_event = events[0]
    log.info(f"ğŸ“„ æµ‹è¯•æ–‡ç« è·å–ï¼Œäº‹ä»¶: {test_event.get('uri', 'N/A')}")
    
    articles = client.fetch_rich_articles_for_event(
        event_uri=test_event['uri'],
        articles_count=2  # åªè·å–2ç¯‡æ–‡ç« è¿›è¡Œæµ‹è¯•
    )
    
    log.info(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
    
    # 7. éªŒè¯æ–‡ç« æ•°æ®ç»“æ„
    if articles:
        article = articles[0]
        log.info("ğŸ“‹ æ–‡ç« æ•°æ®ç»“æ„éªŒè¯:")
        log.info(f"  - URI: {article.get('uri', 'N/A')}")
        log.info(f"  - æ ‡é¢˜: {article.get('title', 'N/A')}")
        log.info(f"  - æ¥æº: {article.get('source', {}).get('name', 'N/A')}")
        log.info(f"  - å‘å¸ƒæ—¶é—´: {article.get('dateTime', 'N/A')}")
        log.info(f"  - æƒ…æ„Ÿåˆ†æ•°: {article.get('sentiment', 'N/A')}")
    
    # 8. æµ‹è¯•APIæ¶ˆè€—ç›‘æ§
    requests_after = client.get_remaining_requests()
    if requests_before and requests_after:
        consumed = requests_before - requests_after
        log.info(f"ğŸ’° APIè¯·æ±‚æ¶ˆè€—: {consumed}")
    
    log.info("ğŸ‰ Phoenix NewsAPIClient æµ‹è¯•å®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = test_phoenix_client()
    if success:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Phoenix ç³»ç»Ÿå‡†å¤‡å°±ç»ªã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒAPIå¯†é’¥ã€‚")
        sys.exit(1) 