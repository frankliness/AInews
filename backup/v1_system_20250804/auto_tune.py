"""
è‡ªåŠ¨è°ƒå‚æ¨¡å—
å®ç°åŸºäºå†å²æ•°æ®çš„å‚æ•°è‡ªåŠ¨ä¼˜åŒ–åŠŸèƒ½
"""
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple
from sqlalchemy import create_engine, text
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import os
import sys
import pathlib
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
BASE_DIR = pathlib.Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))

log = logging.getLogger(__name__)

class AutoTuner:
    """è‡ªåŠ¨è°ƒå‚å™¨"""
    
    def __init__(self, db_url: str, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–è‡ªåŠ¨è°ƒå‚å™¨
        
        Args:
            db_url: æ•°æ®åº“è¿æ¥URL
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.engine = create_engine(db_url)
        self.config_path = config_path
        self.scaler = StandardScaler()
        
    def run(self) -> Dict[str, float]:
        """
        æ‰§è¡Œè‡ªåŠ¨è°ƒå‚ä»»åŠ¡
        
        Returns:
            Dict[str, float]: è°ƒå‚ç»“æœç»Ÿè®¡
        """
        log.info("ğŸš€ å¼€å§‹è‡ªåŠ¨è°ƒå‚...")
        
        # 1. è·å–å†å²æ•°æ®
        df = self._fetch_historical_data()
        if df.empty:
            log.warning("æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„å†å²æ•°æ®")
            return {"status": "no_data"}
        
        log.info(f"ğŸ“Š è·å–åˆ° {len(df)} æ¡å†å²è®°å½•")
        
        # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
        X, y = self._prepare_training_data(df)
        
        # 3. è®­ç»ƒæ¨¡å‹
        model = self._train_model(X, y)
        
        # 4. æå–æ–°å‚æ•°
        new_params = self._extract_parameters(model, X)
        
        # 5. æ›´æ–°é…ç½®æ–‡ä»¶
        stats = self._update_config(new_params)
        
        log.info(f"âœ… è‡ªåŠ¨è°ƒå‚å®Œæˆ: {stats}")
        return stats
    
    def _fetch_historical_data(self) -> pd.DataFrame:
        """è·å–è¿‡å»7å¤©çš„å†å²æ•°æ®"""
        query = """
            SELECT r.id, r.title, r.body, r.published_at, r.url,
                   r.total_articles_24h, r.source_importance, r.wgt,
                   r.likes, r.retweets, r.centroid_sim, r.sentiment,
                   r.score, r.topic_id, r.cluster_size,
                   CASE WHEN d.id IS NOT NULL THEN 1 ELSE 0 END as selected
            FROM raw_events r
            LEFT JOIN daily_cards d ON r.id = d.raw_id
            WHERE r.published_at >= NOW() - INTERVAL '7 DAYS'
              AND r.score IS NOT NULL
        """
        
        try:
            df = pd.read_sql(query, self.engine)
            return df
        except Exception as e:
            log.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        log.info("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # ç‰¹å¾åˆ—
        feature_columns = [
            'total_articles_24h', 'source_importance', 'wgt',
            'likes', 'retweets', 'centroid_sim', 'sentiment'
        ]
        
        # å¡«å……ç¼ºå¤±å€¼
        for col in feature_columns:
            if col in ['likes', 'retweets', 'total_articles_24h']:
                df[col] = df[col].fillna(0)
            elif col == 'sentiment':
                df[col] = df[col].fillna(0)
            else:
                df[col] = df[col].fillna(1)
        
        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        X = df[feature_columns].values
        y = df['selected'].values
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)
        
        log.info(f"ğŸ“ˆ è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {X_scaled.shape[0]} æ ·æœ¬, {X_scaled.shape[1]} ç‰¹å¾")
        log.info(f"ğŸ¯ æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")
        
        return X_scaled, y
    
    def _train_model(self, X: np.ndarray, y: np.ndarray) -> LogisticRegression:
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        log.info("ğŸ¤– è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
        
        # ä½¿ç”¨LogisticRegressionï¼Œè®¾ç½®positive=Trueç¡®ä¿æ­£æ ·æœ¬æƒé‡
        model = LogisticRegression(
            C=0.01,  # æ­£åˆ™åŒ–å¼ºåº¦
            max_iter=1000,
            random_state=42,
            class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        )
        
        model.fit(X, y)
        
        # è¯„ä¼°æ¨¡å‹
        train_score = model.score(X, y)
        log.info(f"ğŸ“Š æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {train_score:.3f}")
        
        return model
    
    def _extract_parameters(self, model: LogisticRegression, X: np.ndarray) -> Dict[str, float]:
        """ä»æ¨¡å‹ç³»æ•°ä¸­æå–æ–°å‚æ•°"""
        log.info("ğŸ” æå–æ–°å‚æ•°...")
        
        # è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ç»å¯¹å€¼ï¼‰
        feature_importance = np.abs(model.coef_[0])
        
        # åæ ‡å‡†åŒ–ç³»æ•°
        feature_importance_original = feature_importance / self.scaler.scale_
        
        # æ˜ å°„åˆ°é…ç½®å‚æ•°
        new_params = {
            "coef_hot": {
                "article": float(feature_importance_original[0]),  # total_articles_24h
                "src_imp": float(feature_importance_original[1]),  # source_importance
                "wgt": float(feature_importance_original[2]),      # wgt
                "likes": float(feature_importance_original[3]),    # likes
                "rts": float(feature_importance_original[4])       # retweets
            },
            "mix_weight": {
                "hot": 0.55,    # ä¿æŒå›ºå®š
                "rep": 0.25,    # ä¿æŒå›ºå®š
                "sent": 0.20    # ä¿æŒå›ºå®š
            }
        }
        
        # å½’ä¸€åŒ–çƒ­åº¦ç³»æ•°
        hot_coefs = list(new_params["coef_hot"].values())
        total_hot = sum(hot_coefs)
        if total_hot > 0:
            for key in new_params["coef_hot"]:
                new_params["coef_hot"][key] = new_params["coef_hot"][key] / total_hot * 10  # ç¼©æ”¾åˆ°åˆç†èŒƒå›´
        
        log.info(f"ğŸ“Š æ–°å‚æ•°æå–å®Œæˆ: {new_params}")
        return new_params
    
    def _update_config(self, new_params: Dict[str, Dict]) -> Dict[str, float]:
        """æ›´æ–°é…ç½®æ–‡ä»¶"""
        log.info("ğŸ’¾ æ›´æ–°é…ç½®æ–‡ä»¶...")
        
        try:
            import ruamel.yaml as ryaml
            
            # è¯»å–ç°æœ‰é…ç½®
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = ryaml.YAML().load(f)
            
            # æ›´æ–°å‚æ•°
            config["coef_hot"] = new_params["coef_hot"]
            config["mix_weight"] = new_params["mix_weight"]
            
            # æ·»åŠ è°ƒå‚æ—¶é—´æˆ³
            config["last_tune_time"] = datetime.now().isoformat()
            
            # å†™å›é…ç½®æ–‡ä»¶
            with open(self.config_path, 'w', encoding='utf-8') as f:
                ryaml.YAML().dump(config, f)
            
            stats = {
                "status": "success",
                "updated_coef_hot": new_params["coef_hot"],
                "updated_mix_weight": new_params["mix_weight"],
                "tune_time": datetime.now().isoformat()
            }
            
            log.info(f"âœ… é…ç½®æ–‡ä»¶æ›´æ–°å®Œæˆ")
            return stats
            
        except Exception as e:
            log.error(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}

def run():
    """è¿è¡Œè‡ªåŠ¨è°ƒå‚ä»»åŠ¡"""
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise RuntimeError("âŒ DATABASE_URL å°šæœªè®¾ç½®ï¼")
    
    tuner = AutoTuner(db_url)
    return tuner.run()

if __name__ == "__main__":
    run() 