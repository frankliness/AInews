"""
Phoenix News Pipeline V2 - ç²¾è‹±æ•°æ®åŠ¨è„‰
é›†æˆä¿¡æºè¿‡æ»¤ã€æ•°é‡æŽ§åˆ¶å’ŒAPIæ¶ˆè€—ç›‘æŽ§çš„æ–°é—»æŠ“å–æµæ°´çº¿
"""

from datetime import datetime, timedelta
import logging
import json
import pandas as pd
import psycopg2
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.hooks.postgres_hook import PostgresHook
from airflow.exceptions import AirflowException
from eventregistry import EventRegistry, GetTrendingConcepts

# å¯¼å…¥æˆ‘ä»¬çš„é…ç½®å’Œå®¢æˆ·ç«¯
# from config.settings import MAX_EVENTS_TO_FETCH, ARTICLES_PER_EVENT  # å·²è¿ç§»è‡³ Airflow Variables
from scraper.newsapi_client import NewsApiClient
from db_utils import bulk_insert_articles
from advanced_scorer import AdvancedScorer

# é…ç½®æ—¥å¿—
log = logging.getLogger(__name__)

# DAG é»˜è®¤å‚æ•°
default_args = {
    'owner': 'phoenix',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def fetch_data_with_monitoring():
    """
    ä¸€ä¸ªé›†æˆäº†é…ç½®ã€ç›‘æŽ§å’Œæ­£ç¡®APIè°ƒç”¨çš„æ•°æ®æŠ“å–ä»»åŠ¡ã€‚
    å®žçŽ°"åŒè·¯å¹¶è¡Œ"æŠ“å–ï¼šçƒ­é—¨äº‹ä»¶ + çªå‘äº‹ä»¶
    """
    # ä½¿ç”¨å¤škeyç®¡ç†ç³»ç»Ÿ
    client = NewsApiClient()

    # --- æœ€ç»ˆç‰ˆï¼šå®Œå…¨ç”±Airflow VariableæŽ§åˆ¶çš„ä¿¡æºè¿‡æ»¤å†³ç­–é€»è¾‘ ---
    import json
    # 1. è¯»å–"æ˜¯å¦å¯ç”¨ç™½åå•"çš„æ€»å¼€å…³
    # default_var='True' æ„å‘³ç€åœ¨å˜é‡ä¸å­˜åœ¨æ—¶ï¼Œé»˜è®¤å¯ç”¨ç™½åå•ï¼Œè¿™æ˜¯ä¸€ä¸ªå®‰å…¨çš„è®¾è®¡
    try:
        enable_whitelist_str = Variable.get("ENABLE_SOURCE_WHITELIST", default_var='True')
        # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼
        should_use_whitelist = json.loads(enable_whitelist_str.lower())
    except Exception as e:
        log.error(f"Failed to parse ENABLE_SOURCE_WHITELIST. Defaulting to True. Error: {e}")
        should_use_whitelist = True
    
    trusted_sources_list = []
    # 2. åªæœ‰åœ¨"æ€»å¼€å…³"æ‰“å¼€æ—¶ï¼Œæ‰åŽ»åŠ è½½ç™½åå•åˆ—è¡¨
    if should_use_whitelist:
        try:
            trusted_sources_json = Variable.get("TRUSTED_SOURCES_WHITELIST", default_var='[]')
            trusted_sources_list = json.loads(trusted_sources_json)
            if not isinstance(trusted_sources_list, list):
                raise ValueError("Variable is not a valid JSON list.")
            log.info(f"âœ… Successfully loaded {len(trusted_sources_list)} trusted sources from Variable.")
            # å¦‚æžœç™½åå•ä¸ºç©ºï¼Œåˆ™å…³é—­å¼€å…³ï¼Œé¿å…APIæŠ¥é”™
            if not trusted_sources_list:
                log.warning("Whitelist is enabled but the list is empty. Disabling whitelist for this run.")
                should_use_whitelist = False
        except (json.JSONDecodeError, ValueError) as e:
            log.error(f"Failed to parse TRUSTED_SOURCES_WHITELIST. Disabling whitelist. Error: {e}")
            should_use_whitelist = False
    
    log.info(f"Final decision on using whitelist: {should_use_whitelist}")
    # --- å†³ç­–é€»è¾‘ç»“æŸ ---

    # ã€æ ¸å¿ƒä¿®å¤ã€‘: è°ƒç”¨ eventregistry å®¢æˆ·ç«¯å®žä¾‹ (er) çš„æ­£ç¡®æ–¹æ³•
    requests_before = client.er.getRemainingAvailableRequests()
    log.info(f"API requests remaining before run: {requests_before}")

    # ä»Ž Airflow Variables èŽ·å–åŠ¨æ€å‚æ•°
    articles_per_event = int(Variable.get("ainews_articles_per_event", default_var=1))
    
    # --- åŒè·¯å¹¶è¡ŒæŠ“å–é€»è¾‘ ---
    # a. è¯»å–å˜é‡
    popular_events_limit = int(Variable.get("ainews_popular_events_limit", default_var=30))
    breaking_events_limit = int(Variable.get("ainews_breaking_events_limit", default_var=20))
    breaking_recent_hours = int(Variable.get("ainews_breaking_recent_hours", default_var=6))
    
    log.info(f"åŒè·¯å¹¶è¡ŒæŠ“å–å‚æ•°: popular_events_limit={popular_events_limit}, breaking_events_limit={breaking_events_limit}, breaking_recent_hours={breaking_recent_hours}")
    
    # b. åŒè·¯è°ƒç”¨
    # çƒ­é—¨äº‹ä»¶æŠ“å–
    log.info("å¼€å§‹æŠ“å–çƒ­é—¨äº‹ä»¶...")
    popular_events = client.fetch_trending_events(
        source_names=trusted_sources_list,
        max_events=popular_events_limit,
        use_whitelist=should_use_whitelist,
        sort_by="size"
    )
    log.info(f"çƒ­é—¨äº‹ä»¶æŠ“å–å®Œæˆï¼ŒèŽ·å¾— {len(popular_events)} ä¸ªäº‹ä»¶")
    
    # çªå‘äº‹ä»¶æŠ“å–
    log.info("å¼€å§‹æŠ“å–çªå‘äº‹ä»¶...")
    breaking_date_start = datetime.utcnow() - timedelta(hours=breaking_recent_hours)
    breaking_events = client.fetch_trending_events(
        source_names=trusted_sources_list,
        max_events=breaking_events_limit,
        use_whitelist=should_use_whitelist,
        sort_by="date",
        date_start=breaking_date_start
    )
    log.info(f"çªå‘äº‹ä»¶æŠ“å–å®Œæˆï¼ŒèŽ·å¾— {len(breaking_events)} ä¸ªäº‹ä»¶")
    
    # c. åˆå¹¶åŽ»é‡
    all_events = popular_events + breaking_events
    log.info(f"åˆå¹¶å‰æ€»äº‹ä»¶æ•°: {len(all_events)}")
    
    # åŸºäºŽäº‹ä»¶çš„ uri å­—æ®µè¿›è¡ŒåŽ»é‡
    seen_uris = set()
    unique_events = []
    for event in all_events:
        if event['uri'] not in seen_uris:
            seen_uris.add(event['uri'])
            unique_events.append(event)
    
    log.info(f"åŽ»é‡åŽäº‹ä»¶æ•°: {len(unique_events)} (åŽ»é‡äº† {len(all_events) - len(unique_events)} ä¸ªé‡å¤äº‹ä»¶)")
    
    # ä½¿ç”¨åŽ»é‡åŽçš„äº‹ä»¶åˆ—è¡¨ç»§ç»­å¤„ç†
    events = unique_events

    all_articles = []
    # 2. å¾ªçŽ¯äº‹ä»¶ï¼Œä½¿ç”¨åŠ¨æ€å‚æ•°èŽ·å–æ–‡ç« 
    for event in events:
        articles = client.fetch_rich_articles_for_event(
            event_uri=event['uri'],
            articles_count=articles_per_event
        )
        # å°†äº‹ä»¶çº§åˆ«çš„å…ƒæ•°æ®ï¼ˆå¦‚æ€»æ–‡ç« æ•°ï¼‰é™„åŠ åˆ°æ¯ç¯‡æ–‡ç« ä¸Šï¼Œä¾¿äºŽåŽç»­æ‰“åˆ†
        for article in articles:
            article['totalArticleCountInEvent'] = event.get('totalArticleCount', 0)
        all_articles.extend(articles)

    log.info(f"Total rich articles fetched: {len(all_articles)}")

    # --- åŽç»­çš„æ‰¹é‡å†™å…¥æ•°æ®åº“é€»è¾‘ ---
    if all_articles:
        # è°ƒç”¨æˆ‘ä»¬æ–°å»ºçš„æ‰¹é‡å†™å…¥å‡½æ•°
        # ç›´æŽ¥è¿žæŽ¥åˆ°Phoenix V2æ•°æ®åº“
        bulk_insert_articles(
            articles=all_articles
        )
    else:
        log.info("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–°æ–‡ç« ï¼Œæ— éœ€å†™å…¥æ•°æ®åº“ã€‚")

    # ã€æ ¸å¿ƒä¿®å¤ã€‘: å†æ¬¡è°ƒç”¨ eventregistry å®¢æˆ·ç«¯å®žä¾‹ (er) çš„æ­£ç¡®æ–¹æ³•
    requests_after = client.er.getRemainingAvailableRequests()
    log.info(f"API requests remaining after run: {requests_after}")

    consumed = requests_before - requests_after
    log.info(f"âœ… API requests consumed in this run: {consumed}")

def update_trending_concepts():
    """
    æ›´æ–°æ¦‚å¿µçƒ­åº¦æ¦œ - æµæ°´çº¿çš„ç¬¬ä¸€ä¸ªæ­¥éª¤
    
    ç”Ÿäº§ç‰ˆæœ¬ï¼šå¦‚æžœAPIè°ƒç”¨å¤±è´¥ï¼Œå°†æŠ›å‡ºAirflowExceptionï¼Œ
    ç¡®ä¿ä»»åŠ¡è¿›å…¥FAILED/UP_FOR_RETRYçŠ¶æ€ï¼Œæ–¹ä¾¿å‘Šè­¦ä¸Žé‡è¯•ã€‚
    """
    log.info("å¼€å§‹æ›´æ–°æ¦‚å¿µçƒ­åº¦æ¦œ...")
    
    # ä½¿ç”¨å¤škeyç®¡ç†ç³»ç»Ÿ
    client = NewsApiClient()
    er = client.er
    
    try:
        # è°ƒç”¨GetTrendingConcepts APIèŽ·å–æœ€æ–°çš„æ¦‚å¿µçƒ­åº¦æ¦œ
        query = GetTrendingConcepts(
            source="news",      # æˆ‘ä»¬åªå…³å¿ƒæ–°é—»ä¸­çš„çƒ­ç‚¹
            count=200,          # å»ºè®®èŽ·å–æ¯”100ç¨å¤šçš„æ•°é‡ï¼Œä»¥å¢žåŠ åŒ¹é…æ¦‚çŽ‡
            conceptType=["person", "org", "loc"]  # æˆ‘ä»¬å…³å¿ƒçš„ä¸‰ç§å®žä½“ç±»åž‹
        )
        result = client._execute_api_call(er.execQuery, query)
        
        # å¤„ç†APIå“åº”ï¼Œæž„å»ºæ¦‚å¿µçƒ­åº¦å­—å…¸
        # EventRegistry APIè¿”å›žçš„æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(result, list):
            items = result
        elif isinstance(result, dict):
            items = result.get("trendingConcepts") or result.get("results") or []
        else:
            raise ValueError("Unexpected API payload structure")
        
        if not items:
            raise ValueError("No trending concepts found in API response")
        
        concepts_to_insert = [
            (c["uri"],
             float(c["trendingScore"]["news"]["score"]),
             datetime.utcnow())
            for c in items
        ]
        
        log.info(f"APIè°ƒç”¨æˆåŠŸï¼ŒèŽ·å–åˆ° {len(concepts_to_insert)} ä¸ªçƒ­é—¨æ¦‚å¿µ")
        
    except Exception as exc:
        # è®°å½•é”™è¯¯å¹¶è®©Airflowå¤„ç†å¤±è´¥/é‡è¯•é€»è¾‘
        log.exception("âŒ Failed to fetch trending concepts: %s", exc)
        raise AirflowException("Trending concept update failed") from exc
    
    # ------------------ UPSERT åˆ° Postgres ------------------
    db_config = {
        'host': 'postgres-phoenix',
        'port': 5432,
        'database': 'phoenix_db',
        'user': 'phoenix_user',
        'password': 'phoenix_pass',
        'options': '-c timezone=Asia/Shanghai'  # è®¾ç½®è¿žæŽ¥æ—¶åŒº
    }
    
    try:
        with psycopg2.connect(**db_config) as conn:
            with conn.cursor() as cur:
                from psycopg2.extras import execute_values
                execute_values(
                    cur,
                    """
                    INSERT INTO trending_concepts (uri, score, updated_at)
                    VALUES %s
                    ON CONFLICT (uri) DO UPDATE
                      SET score = EXCLUDED.score,
                          updated_at = EXCLUDED.updated_at;
                    """,
                    concepts_to_insert,
                    page_size=100,
                )
            conn.commit()
            log.info("âœ… Upserted %d trending concepts", len(concepts_to_insert))
    except Exception as e:
        log.error(f"âŒ æ•°æ®åº“UPSERTæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def process_and_score_articles():
    """
    å¤„ç†å’Œè¯„åˆ†æ–‡ç«  - æ ¸å¿ƒçš„"æ‰“åˆ†ä¸Žæ›´æ–°"ä»»åŠ¡
    """
    log.info("å¼€å§‹å¤„ç†å’Œè¯„åˆ†æ–‡ç« ...")
    
    # æ•°æ®åº“è¿žæŽ¥é…ç½®
    db_config = {
        'host': 'postgres-phoenix',
        'port': 5432,
        'database': 'phoenix_db',
        'user': 'phoenix_user',
        'password': 'phoenix_pass'
    }
    
    try:
        with psycopg2.connect(**db_config) as conn:
            # 1. åŠ è½½å¤–éƒ¨å¤§è„‘ï¼šä»ŽV2æ•°æ®åº“çš„trending_conceptsè¡¨ä¸­è¯»å–æ‰€æœ‰æ¦‚å¿µçƒ­åº¦
            log.info("åŠ è½½æ¦‚å¿µçƒ­åº¦æ•°æ®...")
            with conn.cursor() as cur:
                cur.execute("SELECT uri, score FROM trending_concepts")
                concepts_data = cur.fetchall()
                concepts_dict = {uri: score for uri, score in concepts_data}
                log.info(f"åŠ è½½äº† {len(concepts_dict)} ä¸ªæ¦‚å¿µçš„çƒ­åº¦æ•°æ®")
            
            # 2. èŽ·å–å¾…å¤„ç†æ•°æ®ï¼šä»Žraw_eventsè¡¨ä¸­æŸ¥è¯¢æ‰€æœ‰final_score_v2 IS NULLçš„æ–°æ–‡ç« 
            log.info("æŸ¥è¯¢å¾…å¤„ç†çš„æ–°æ–‡ç« ...")
            query_sql = """
            SELECT id, title, body, url, published_at, sentiment, source_name, 
                   source_importance, event_uri, total_articles_in_event, concepts
            FROM raw_events 
            WHERE final_score_v2 IS NULL
            ORDER BY collected_at DESC
            """
            
            articles_df = pd.read_sql_query(query_sql, conn)
            log.info(f"æ‰¾åˆ° {len(articles_df)} ç¯‡å¾…å¤„ç†æ–‡ç« ")
            
            if articles_df.empty:
                log.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°æ–‡ç« ï¼Œä»»åŠ¡å®Œæˆ")
                return
            
            # å¤„ç†conceptsåˆ— - å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
            articles_df['concepts'] = articles_df['concepts'].apply(
                lambda x: json.loads(x) if x and isinstance(x, str) else x
            )
            
            # 3. è°ƒç”¨æ™ºæ…§æ ¸å¿ƒï¼šå®žä¾‹åŒ–AdvancedScorerå¹¶è¿›è¡Œæ‰“åˆ†
            log.info("åˆå§‹åŒ–AdvancedScorerå¹¶å¼€å§‹æ‰“åˆ†...")
            scorer = AdvancedScorer(concepts_dict)
            scored_df = scorer.score(articles_df)
            
            # 4. ç»“æžœå†™å›žæ•°æ®åº“ï¼šæ›´æ–°raw_eventsè¡¨ä¸­å¯¹åº”çš„è¡Œ
            log.info("å°†æ‰“åˆ†ç»“æžœå†™å›žæ•°æ®åº“...")
            with conn.cursor() as cur:
                update_sql = """
                UPDATE raw_events SET
                    entity_hot_score = %s,
                    hot_score_v2 = %s,
                    rep_score_v2 = %s,
                    sent_score_v2 = %s,
                    hot_norm = %s,
                    rep_norm = %s,
                    sent_norm = %s,
                    final_score_v2 = %s
                WHERE id = %s
                """
                
                update_data = []
                for _, row in scored_df.iterrows():
                    update_data.append((
                        float(row['entity_hot_score']),
                        float(row['hot_score_v2']),
                        float(row['rep_score_v2']),
                        float(row['sent_score_v2']),
                        float(row['hot_norm']),
                        float(row['rep_norm']),
                        float(row['sent_norm']),
                        float(row['final_score_v2']),
                        row['id']
                    ))
                
                cur.executemany(update_sql, update_data)
                conn.commit()
                
                log.info(f"âœ… æˆåŠŸæ›´æ–° {len(update_data)} ç¯‡æ–‡ç« çš„åˆ†æ•°")
                
                # è¾“å‡ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                avg_score = scored_df['final_score_v2'].mean()
                max_score = scored_df['final_score_v2'].max()
                log.info(f"ðŸ“Š æ‰“åˆ†ç»Ÿè®¡ - å¹³å‡åˆ†: {avg_score:.4f}, æœ€é«˜åˆ†: {max_score:.4f}")
    
    except Exception as e:
        log.error(f"âŒ å¤„ç†å’Œè¯„åˆ†æ–‡ç« æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise



# åˆ›å»ºDAG
dag = DAG(
    'ingestion_scoring_pipeline',
    default_args=default_args,
    description='é«˜é¢‘è¿è¡Œçš„æ–°é—»æŠ“å–ä¸Žé«˜çº§æ‰“åˆ†æµæ°´çº¿',
    schedule_interval='0 2,9,14 * * *',  # åŒ—äº¬æ—¶é—´10:00, 17:00, 22:00 (UTC 2:00, 9:00, 14:00)
    catchup=False,
    tags=['phoenix', 'ingestion', 'scoring'],
)

# å®šä¹‰ä»»åŠ¡
update_trending_concepts_task = PythonOperator(
    task_id='update_trending_concepts',
    python_callable=update_trending_concepts,
    dag=dag,
)

fetch_task = PythonOperator(
    task_id='fetch_data_with_monitoring',
    python_callable=fetch_data_with_monitoring,
    dag=dag,
)

process_task = PythonOperator(
    task_id='process_and_score_articles',
    python_callable=process_and_score_articles,
    dag=dag,
)

# è®¾ç½®ä»»åŠ¡ä¾èµ–
update_trending_concepts_task >> fetch_task >> process_task 