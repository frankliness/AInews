"""
ä¸»é¢˜èšç±»æ¨¡å—
å®ç°EventRegistryäº‹ä»¶èšç±»å’Œç»†åˆ†åŠŸèƒ½
"""
import pandas as pd
import numpy as np
import logging
from typing import List, Dict, Optional
from sqlalchemy import create_engine, text
from sklearn.cluster import MiniBatchKMeans
from sklearn.preprocessing import StandardScaler
import os
import sys
import pathlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
BASE_DIR = pathlib.Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))

from utils.text import cosine, parse_embedding

log = logging.getLogger(__name__)

class TopicClusterer:
    """ä¸»é¢˜èšç±»å™¨"""
    
    def __init__(self, db_url: str, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–èšç±»å™¨
        
        Args:
            db_url: æ•°æ®åº“è¿æ¥URL
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.engine = create_engine(db_url)
        self.config = self._load_config(config_path)
        self.n_clusters = self.config.get("clustering", {}).get("n_clusters", 150)
        self.max_cluster_size = self.config.get("clustering", {}).get("max_cluster_size", 50)
        
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            import ruamel.yaml as ryaml
            with open(config_path, 'r', encoding='utf-8') as f:
                return ryaml.YAML().load(f)
        except Exception as e:
            log.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}
    
    def run(self) -> Dict[str, int]:
        """
        æ‰§è¡Œèšç±»ä»»åŠ¡
        
        Returns:
            Dict[str, int]: èšç±»ç»Ÿè®¡ä¿¡æ¯
        """
        log.info("ğŸš€ å¼€å§‹ä¸»é¢˜èšç±»...")
        
        # 1. è·å–24å°æ—¶å†…çš„æ•°æ®
        df = self._fetch_recent_data()
        if df.empty:
            log.warning("æ²¡æœ‰æ‰¾åˆ°24å°æ—¶å†…çš„æ•°æ®")
            return {"total_records": 0, "clustered_records": 0}
        
        log.info(f"ğŸ“Š è·å–åˆ° {len(df)} æ¡è®°å½•")
        
        # 2. ä¸€çº§èšç±»ï¼šä½¿ç”¨EventRegistryçš„event_id
        df = self._primary_clustering(df)
        
        # 3. äºŒçº§èšç±»ï¼šå¯¹è¶…å¤§äº‹ä»¶è¿›è¡Œç»†åˆ†
        df = self._secondary_clustering(df)
        
        # 4. æ›´æ–°æ•°æ®åº“
        stats = self._update_database(df)
        
        log.info(f"âœ… èšç±»å®Œæˆ: {stats}")
        return stats
    
    def _fetch_recent_data(self) -> pd.DataFrame:
        """è·å–24å°æ—¶å†…çš„æ•°æ®"""
        query = """
            SELECT id, event_id, embedding, published_at
            FROM raw_events
            WHERE published_at >= NOW() - INTERVAL '24 HOURS'
              AND event_id IS NOT NULL
        """
        
        try:
            df = pd.read_sql(query, self.engine)
            return df
        except Exception as e:
            log.error(f"è·å–æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _primary_clustering(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¸€çº§èšç±»ï¼šä½¿ç”¨EventRegistryçš„event_id"""
        log.info("ğŸ” æ‰§è¡Œä¸€çº§èšç±»...")
        
        # å°†event_idè½¬æ¢ä¸ºæ•°å­—ä½œä¸ºtopic_id
        # ä½¿ç”¨hashå‡½æ•°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—
        df["topic_id"] = df["event_id"].apply(lambda x: hash(x) % (2**63 - 1))
        df["centroid_sim"] = 1.0  # åŒä¸€äº‹ä»¶çš„ç›¸ä¼¼åº¦ä¸º1
        
        # ç»Ÿè®¡cluster_size
        size_counts = df.groupby("topic_id").size().rename("cluster_size")
        df = df.join(size_counts, on="topic_id")
        
        log.info(f"ğŸ“ˆ ä¸€çº§èšç±»å®Œæˆï¼Œå…± {df['topic_id'].nunique()} ä¸ªä¸»é¢˜")
        return df
    
    def _secondary_clustering(self, df: pd.DataFrame) -> pd.DataFrame:
        """äºŒçº§èšç±»ï¼šå¯¹è¶…å¤§äº‹ä»¶è¿›è¡Œç»†åˆ†"""
        log.info("ğŸ” æ‰§è¡ŒäºŒçº§èšç±»...")
        
        # æ‰¾å‡ºè¶…å¤§äº‹ä»¶
        big_clusters = df[df.cluster_size > self.max_cluster_size]
        
        if big_clusters.empty:
            log.info("æ²¡æœ‰è¶…å¤§äº‹ä»¶éœ€è¦ç»†åˆ†")
            return df
        
        log.info(f"ğŸ“Š å‘ç° {len(big_clusters)} ä¸ªè¶…å¤§äº‹ä»¶ï¼Œå¼€å§‹ç»†åˆ†...")
        
        # å¯¹æ¯ä¸ªè¶…å¤§äº‹ä»¶è¿›è¡Œç»†åˆ†
        for topic_id in big_clusters["topic_id"].unique():
            cluster_data = df[df["topic_id"] == topic_id]
            
            # è§£æembedding
            embeddings = []
            valid_indices = []
            
            for idx, row in cluster_data.iterrows():
                embedding = parse_embedding(row["embedding"])
                if embedding:
                    embeddings.append(embedding)
                    valid_indices.append(idx)
            
            if len(embeddings) < 3:
                continue  # è·³è¿‡å¤ªå°çš„èšç±»
            
            # æ‰§è¡ŒK-meansèšç±»
            k = min(self.n_clusters // 10, len(embeddings) // 3, 10)  # é™åˆ¶èšç±»æ•°
            if k < 2:
                continue
            
            try:
                km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=100)
                km.fit(embeddings)
                
                # æ›´æ–°topic_idå’Œç›¸ä¼¼åº¦
                for i, idx in enumerate(valid_indices):
                    new_topic_id = f"{topic_id}_sub_{km.labels_[i]}"
                    df.loc[idx, "topic_id"] = new_topic_id
                    
                    # è®¡ç®—ä¸èšç±»ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
                    centroid = km.cluster_centers_[km.labels_[i]]
                    similarity = cosine(embeddings[i], centroid)
                    df.loc[idx, "centroid_sim"] = similarity
                    
            except Exception as e:
                log.error(f"ç»†åˆ†èšç±» {topic_id} å¤±è´¥: {e}")
                continue
        
        # é‡æ–°è®¡ç®—cluster_size
        size_counts = df.groupby("topic_id").size().rename("cluster_size")
        df = df.drop(columns=["cluster_size"]).join(size_counts, on="topic_id")
        
        log.info(f"âœ… äºŒçº§èšç±»å®Œæˆï¼Œç»†åˆ†åå…± {df['topic_id'].nunique()} ä¸ªä¸»é¢˜")
        return df
    
    def _update_database(self, df: pd.DataFrame) -> Dict[str, int]:
        """æ›´æ–°æ•°æ®åº“"""
        log.info("ğŸ’¾ æ›´æ–°æ•°æ®åº“...")
        
        # å‡†å¤‡æ›´æ–°æ•°æ®
        update_data = df[["id", "topic_id", "cluster_size", "centroid_sim"]].copy()
        
        # å†™å…¥ä¸´æ—¶è¡¨
        update_data.to_sql("tmp_cluster", self.engine, if_exists="replace", index=False)
        
        # æ‰§è¡Œæ›´æ–°
        with self.engine.begin() as conn:
            result = conn.execute(text("""
                UPDATE raw_events r
                SET topic_id      = t.topic_id,
                    cluster_size  = t.cluster_size,
                    centroid_sim  = t.centroid_sim
                FROM tmp_cluster t 
                WHERE r.id = t.id
            """))
            
            # åˆ é™¤ä¸´æ—¶è¡¨
            conn.execute(text("DROP TABLE IF EXISTS tmp_cluster"))
            
            updated_count = result.rowcount
        
        stats = {
            "total_records": len(df),
            "clustered_records": updated_count,
            "unique_topics": df["topic_id"].nunique()
        }
        
        log.info(f"âœ… æ•°æ®åº“æ›´æ–°å®Œæˆ: {stats}")
        return stats

def run():
    """è¿è¡Œèšç±»ä»»åŠ¡"""
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise RuntimeError("âŒ DATABASE_URL å°šæœªè®¾ç½®ï¼")
    
    clusterer = TopicClusterer(db_url)
    return clusterer.run()

if __name__ == "__main__":
    run() 