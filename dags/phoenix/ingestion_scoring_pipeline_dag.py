"""
Phoenix News Pipeline V2 - ç²¾è‹±æ•°æ®åŠ¨è„‰
é›†æˆä¿¡æºè¿‡æ»¤ã€æ•°é‡æ§åˆ¶å’ŒAPIæ¶ˆè€—ç›‘æ§çš„æ–°é—»æŠ“å–æµæ°´çº¿
"""

from datetime import datetime, timedelta
import logging
import json
import pandas as pd
import psycopg2
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.hooks.postgres_hook import PostgresHook
from airflow.exceptions import AirflowException
from eventregistry import EventRegistry, GetTrendingConcepts

# å¯¼å…¥æˆ‘ä»¬çš„é…ç½®å’Œå®¢æˆ·ç«¯
from config.settings import MAX_EVENTS_TO_FETCH, ARTICLES_PER_EVENT
from scraper.newsapi_client import NewsApiClient
from db_utils import bulk_insert_articles
from advanced_scorer import AdvancedScorer

# é…ç½®æ—¥å¿—
log = logging.getLogger(__name__)

# DAG é»˜è®¤å‚æ•°
default_args = {
    'owner': 'phoenix',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def fetch_data_with_monitoring():
    """
    ä¸€ä¸ªé›†æˆäº†é…ç½®ã€ç›‘æ§å’Œæ­£ç¡®APIè°ƒç”¨çš„æ•°æ®æŠ“å–ä»»åŠ¡ã€‚
    """
    # ä»Airflow Variableè·å–APIå¯†é’¥
    api_key = Variable.get("EVENTREGISTRY_APIKEY")
    client = NewsApiClient(api_key=api_key)

    # --- æœ€ç»ˆç‰ˆï¼šå®Œå…¨ç”±Airflow Variableæ§åˆ¶çš„ä¿¡æºè¿‡æ»¤å†³ç­–é€»è¾‘ ---
    import json
    # 1. è¯»å–"æ˜¯å¦å¯ç”¨ç™½åå•"çš„æ€»å¼€å…³
    # default_var='True' æ„å‘³ç€åœ¨å˜é‡ä¸å­˜åœ¨æ—¶ï¼Œé»˜è®¤å¯ç”¨ç™½åå•ï¼Œè¿™æ˜¯ä¸€ä¸ªå®‰å…¨çš„è®¾è®¡
    try:
        enable_whitelist_str = Variable.get("ENABLE_SOURCE_WHITELIST", default_var='True')
        # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼
        should_use_whitelist = json.loads(enable_whitelist_str.lower())
    except Exception as e:
        log.error(f"Failed to parse ENABLE_SOURCE_WHITELIST. Defaulting to True. Error: {e}")
        should_use_whitelist = True
    
    trusted_sources_list = []
    # 2. åªæœ‰åœ¨"æ€»å¼€å…³"æ‰“å¼€æ—¶ï¼Œæ‰å»åŠ è½½ç™½åå•åˆ—è¡¨
    if should_use_whitelist:
        try:
            trusted_sources_json = Variable.get("TRUSTED_SOURCES_WHITELIST", default_var='[]')
            trusted_sources_list = json.loads(trusted_sources_json)
            if not isinstance(trusted_sources_list, list):
                raise ValueError("Variable is not a valid JSON list.")
            log.info(f"âœ… Successfully loaded {len(trusted_sources_list)} trusted sources from Variable.")
            # å¦‚æœç™½åå•ä¸ºç©ºï¼Œåˆ™å…³é—­å¼€å…³ï¼Œé¿å…APIæŠ¥é”™
            if not trusted_sources_list:
                log.warning("Whitelist is enabled but the list is empty. Disabling whitelist for this run.")
                should_use_whitelist = False
        except (json.JSONDecodeError, ValueError) as e:
            log.error(f"Failed to parse TRUSTED_SOURCES_WHITELIST. Disabling whitelist. Error: {e}")
            should_use_whitelist = False
    
    log.info(f"Final decision on using whitelist: {should_use_whitelist}")
    # --- å†³ç­–é€»è¾‘ç»“æŸ ---

    # ã€æ ¸å¿ƒä¿®å¤ã€‘: è°ƒç”¨ eventregistry å®¢æˆ·ç«¯å®ä¾‹ (er) çš„æ­£ç¡®æ–¹æ³•
    requests_before = client.er.getRemainingAvailableRequests()
    log.info(f"API requests remaining before run: {requests_before}")

    # å°†æœ€ç»ˆçš„å†³ç­–å’Œç™½åå•åˆ—è¡¨ä¼ é€’ç»™å®¢æˆ·ç«¯
    events = client.fetch_trending_events(
        source_names=trusted_sources_list,
        max_events=MAX_EVENTS_TO_FETCH,
        use_whitelist=should_use_whitelist # << ä½¿ç”¨æˆ‘ä»¬è®¡ç®—å‡ºçš„æœ€ç»ˆå†³ç­–
    )

    all_articles = []
    # 2. å¾ªç¯äº‹ä»¶ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ–‡ç« æ•°é™åˆ¶ï¼Œè·å–æ–‡ç« 
    for event in events:
        articles = client.fetch_rich_articles_for_event(
            event_uri=event['uri'],
            articles_count=ARTICLES_PER_EVENT
        )
        # å°†äº‹ä»¶çº§åˆ«çš„å…ƒæ•°æ®ï¼ˆå¦‚æ€»æ–‡ç« æ•°ï¼‰é™„åŠ åˆ°æ¯ç¯‡æ–‡ç« ä¸Šï¼Œä¾¿äºåç»­æ‰“åˆ†
        for article in articles:
            article['totalArticleCountInEvent'] = event.get('totalArticleCount', 0)
        all_articles.extend(articles)

    log.info(f"Total rich articles fetched: {len(all_articles)}")

    # --- åç»­çš„æ‰¹é‡å†™å…¥æ•°æ®åº“é€»è¾‘ ---
    if all_articles:
        # è°ƒç”¨æˆ‘ä»¬æ–°å»ºçš„æ‰¹é‡å†™å…¥å‡½æ•°
        # ç›´æ¥è¿æ¥åˆ°Phoenix V2æ•°æ®åº“
        bulk_insert_articles(
            articles=all_articles
        )
    else:
        log.info("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–°æ–‡ç« ï¼Œæ— éœ€å†™å…¥æ•°æ®åº“ã€‚")

    # ã€æ ¸å¿ƒä¿®å¤ã€‘: å†æ¬¡è°ƒç”¨ eventregistry å®¢æˆ·ç«¯å®ä¾‹ (er) çš„æ­£ç¡®æ–¹æ³•
    requests_after = client.er.getRemainingAvailableRequests()
    log.info(f"API requests remaining after run: {requests_after}")

    consumed = requests_before - requests_after
    log.info(f"âœ… API requests consumed in this run: {consumed}")

def update_trending_concepts():
    """
    æ›´æ–°æ¦‚å¿µçƒ­åº¦æ¦œ - æµæ°´çº¿çš„ç¬¬ä¸€ä¸ªæ­¥éª¤
    
    ç”Ÿäº§ç‰ˆæœ¬ï¼šå¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œå°†æŠ›å‡ºAirflowExceptionï¼Œ
    ç¡®ä¿ä»»åŠ¡è¿›å…¥FAILED/UP_FOR_RETRYçŠ¶æ€ï¼Œæ–¹ä¾¿å‘Šè­¦ä¸é‡è¯•ã€‚
    """
    log.info("å¼€å§‹æ›´æ–°æ¦‚å¿µçƒ­åº¦æ¦œ...")
    
    # ä»Airflow Variableè·å–APIå¯†é’¥
    api_key = Variable.get("EVENTREGISTRY_APIKEY")
    er = EventRegistry(apiKey=api_key, allowUseOfArchive=False)
    
    try:
        # è°ƒç”¨GetTrendingConcepts APIè·å–æœ€æ–°çš„æ¦‚å¿µçƒ­åº¦æ¦œ
        query = GetTrendingConcepts(
            source="news",      # æˆ‘ä»¬åªå…³å¿ƒæ–°é—»ä¸­çš„çƒ­ç‚¹
            count=200,          # å»ºè®®è·å–æ¯”100ç¨å¤šçš„æ•°é‡ï¼Œä»¥å¢åŠ åŒ¹é…æ¦‚ç‡
            conceptType=["person", "org", "loc"]  # æˆ‘ä»¬å…³å¿ƒçš„ä¸‰ç§å®ä½“ç±»å‹
        )
        result = er.execQuery(query)
        
        # å¤„ç†APIå“åº”ï¼Œæ„å»ºæ¦‚å¿µçƒ­åº¦å­—å…¸
        # EventRegistry APIè¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(result, list):
            items = result
        elif isinstance(result, dict):
            items = result.get("trendingConcepts") or result.get("results") or []
        else:
            raise ValueError("Unexpected API payload structure")
        
        if not items:
            raise ValueError("No trending concepts found in API response")
        
        concepts_to_insert = [
            (c["uri"],
             float(c["trendingScore"]["news"]["score"]),
             datetime.utcnow())
            for c in items
        ]
        
        log.info(f"APIè°ƒç”¨æˆåŠŸï¼Œè·å–åˆ° {len(concepts_to_insert)} ä¸ªçƒ­é—¨æ¦‚å¿µ")
        
    except Exception as exc:
        # è®°å½•é”™è¯¯å¹¶è®©Airflowå¤„ç†å¤±è´¥/é‡è¯•é€»è¾‘
        log.exception("âŒ Failed to fetch trending concepts: %s", exc)
        raise AirflowException("Trending concept update failed") from exc
    
    # ------------------ UPSERT åˆ° Postgres ------------------
    db_config = {
        'host': 'postgres-phoenix',
        'port': 5432,
        'database': 'phoenix_db',
        'user': 'phoenix_user',
        'password': 'phoenix_pass',
        'options': '-c timezone=Asia/Shanghai'  # è®¾ç½®è¿æ¥æ—¶åŒº
    }
    
    try:
        with psycopg2.connect(**db_config) as conn:
            with conn.cursor() as cur:
                from psycopg2.extras import execute_values
                execute_values(
                    cur,
                    """
                    INSERT INTO trending_concepts (uri, score, updated_at)
                    VALUES %s
                    ON CONFLICT (uri) DO UPDATE
                      SET score = EXCLUDED.score,
                          updated_at = EXCLUDED.updated_at;
                    """,
                    concepts_to_insert,
                    page_size=100,
                )
            conn.commit()
            log.info("âœ… Upserted %d trending concepts", len(concepts_to_insert))
    except Exception as e:
        log.error(f"âŒ æ•°æ®åº“UPSERTæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def process_and_score_articles():
    """
    å¤„ç†å’Œè¯„åˆ†æ–‡ç«  - æ ¸å¿ƒçš„"æ‰“åˆ†ä¸æ›´æ–°"ä»»åŠ¡
    """
    log.info("å¼€å§‹å¤„ç†å’Œè¯„åˆ†æ–‡ç« ...")
    
    # æ•°æ®åº“è¿æ¥é…ç½®
    db_config = {
        'host': 'postgres-phoenix',
        'port': 5432,
        'database': 'phoenix_db',
        'user': 'phoenix_user',
        'password': 'phoenix_pass'
    }
    
    try:
        with psycopg2.connect(**db_config) as conn:
            # 1. åŠ è½½å¤–éƒ¨å¤§è„‘ï¼šä»V2æ•°æ®åº“çš„trending_conceptsè¡¨ä¸­è¯»å–æ‰€æœ‰æ¦‚å¿µçƒ­åº¦
            log.info("åŠ è½½æ¦‚å¿µçƒ­åº¦æ•°æ®...")
            with conn.cursor() as cur:
                cur.execute("SELECT uri, score FROM trending_concepts")
                concepts_data = cur.fetchall()
                concepts_dict = {uri: score for uri, score in concepts_data}
                log.info(f"åŠ è½½äº† {len(concepts_dict)} ä¸ªæ¦‚å¿µçš„çƒ­åº¦æ•°æ®")
            
            # 2. è·å–å¾…å¤„ç†æ•°æ®ï¼šä»raw_eventsè¡¨ä¸­æŸ¥è¯¢æ‰€æœ‰final_score_v2 IS NULLçš„æ–°æ–‡ç« 
            log.info("æŸ¥è¯¢å¾…å¤„ç†çš„æ–°æ–‡ç« ...")
            query_sql = """
            SELECT id, title, body, url, published_at, sentiment, source_name, 
                   source_importance, event_uri, total_articles_in_event, concepts
            FROM raw_events 
            WHERE final_score_v2 IS NULL
            ORDER BY collected_at DESC
            """
            
            articles_df = pd.read_sql_query(query_sql, conn)
            log.info(f"æ‰¾åˆ° {len(articles_df)} ç¯‡å¾…å¤„ç†æ–‡ç« ")
            
            if articles_df.empty:
                log.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°æ–‡ç« ï¼Œä»»åŠ¡å®Œæˆ")
                return
            
            # å¤„ç†conceptsåˆ— - å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
            articles_df['concepts'] = articles_df['concepts'].apply(
                lambda x: json.loads(x) if x and isinstance(x, str) else x
            )
            
            # 3. è°ƒç”¨æ™ºæ…§æ ¸å¿ƒï¼šå®ä¾‹åŒ–AdvancedScorerå¹¶è¿›è¡Œæ‰“åˆ†
            log.info("åˆå§‹åŒ–AdvancedScorerå¹¶å¼€å§‹æ‰“åˆ†...")
            scorer = AdvancedScorer(concepts_dict)
            scored_df = scorer.score(articles_df)
            
            # 4. ç»“æœå†™å›æ•°æ®åº“ï¼šæ›´æ–°raw_eventsè¡¨ä¸­å¯¹åº”çš„è¡Œ
            log.info("å°†æ‰“åˆ†ç»“æœå†™å›æ•°æ®åº“...")
            with conn.cursor() as cur:
                update_sql = """
                UPDATE raw_events SET
                    entity_hot_score = %s,
                    hot_score_v2 = %s,
                    rep_score_v2 = %s,
                    sent_score_v2 = %s,
                    hot_norm = %s,
                    rep_norm = %s,
                    sent_norm = %s,
                    final_score_v2 = %s
                WHERE id = %s
                """
                
                update_data = []
                for _, row in scored_df.iterrows():
                    update_data.append((
                        float(row['entity_hot_score']),
                        float(row['hot_score_v2']),
                        float(row['rep_score_v2']),
                        float(row['sent_score_v2']),
                        float(row['hot_norm']),
                        float(row['rep_norm']),
                        float(row['sent_norm']),
                        float(row['final_score_v2']),
                        row['id']
                    ))
                
                cur.executemany(update_sql, update_data)
                conn.commit()
                
                log.info(f"âœ… æˆåŠŸæ›´æ–° {len(update_data)} ç¯‡æ–‡ç« çš„åˆ†æ•°")
                
                # è¾“å‡ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                avg_score = scored_df['final_score_v2'].mean()
                max_score = scored_df['final_score_v2'].max()
                log.info(f"ğŸ“Š æ‰“åˆ†ç»Ÿè®¡ - å¹³å‡åˆ†: {avg_score:.4f}, æœ€é«˜åˆ†: {max_score:.4f}")
    
    except Exception as e:
        log.error(f"âŒ å¤„ç†å’Œè¯„åˆ†æ–‡ç« æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise



# åˆ›å»ºDAG
dag = DAG(
    'ingestion_scoring_pipeline',
    default_args=default_args,
    description='é«˜é¢‘è¿è¡Œçš„æ–°é—»æŠ“å–ä¸é«˜çº§æ‰“åˆ†æµæ°´çº¿',
    schedule_interval='0 2,9,14 * * *',  # åŒ—äº¬æ—¶é—´10:00, 17:00, 22:00 (UTC 2:00, 9:00, 14:00)
    catchup=False,
    tags=['phoenix', 'ingestion', 'scoring'],
)

# å®šä¹‰ä»»åŠ¡
update_trending_concepts_task = PythonOperator(
    task_id='update_trending_concepts',
    python_callable=update_trending_concepts,
    dag=dag,
)

fetch_task = PythonOperator(
    task_id='fetch_data_with_monitoring',
    python_callable=fetch_data_with_monitoring,
    dag=dag,
)

process_task = PythonOperator(
    task_id='process_and_score_articles',
    python_callable=process_and_score_articles,
    dag=dag,
)

# è®¾ç½®ä»»åŠ¡ä¾èµ–
update_trending_concepts_task >> fetch_task >> process_task 