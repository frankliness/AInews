"""
Airflow DAG â€“ æ¯å¤© 3 æ¬¡æŠ“å–å¤šæºæ–°é—» â†’ Postgres raw_events
æ”¯æŒ Reutersã€BBCã€NYTimesã€APNewsã€Bloombergã€TheGuardianã€CNNã€FTã€SkyNewsã€ElPaisã€SCMPã€AlJazeeraã€TheVerge ç­‰13ä¸ªæº
"""
from __future__ import annotations
import os, sys, pathlib, logging
from datetime import datetime, timedelta

import pytz
import psycopg2
from airflow import DAG
from airflow.operators.python import PythonOperator

BASE_DIR = pathlib.Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR))   # è®© Airflow æ‰¾åˆ° scraper

from scraper import (
    ReutersScraper, BBCScraper, NYTimesScraper, APNewsScraper, 
    BloombergScraper, TheGuardianScraper, CNNScraper, FTScraper,
    SkyNewsScraper, ElPaisScraper, SCMPScraper, AlJazeeraScraper,
    TheVergeScraper
)  # noqa: E402

log = logging.getLogger(__name__)
SQL = """
INSERT INTO raw_events
(source,title,body,published_at,url,likes,retweets,event_id,embedding,total_articles_24h,source_importance,wgt,sentiment,collected_at)
VALUES (%(source)s,%(title)s,%(body)s,%(published_at)s,%(url)s,%(likes)s,%(retweets)s,%(event_id)s,%(embedding)s,%(total_articles_24h)s,%(source_importance)s,%(weight)s,%(sentiment)s,NOW())
ON CONFLICT (url) DO NOTHING;
"""

# Asia/Shanghai æ—¶åŒº
SH_TZ = pytz.timezone('Asia/Shanghai')

# ä»»åŠ¡ä¸»ä½“
def _fetch_eventregistry_news():
    """å¤šæºæ–°é—»æŠ“å–ä»»åŠ¡"""
    db_url = os.environ.get("DATABASE_URL")
    if not db_url:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ DATABASE_URL")
    
    try:
        with psycopg2.connect(db_url) as conn:
            total = 0
            success_count = 0
            error_count = 0
            
            # æ‰€æœ‰æ–°é—»æºæŠ“å–å™¨
            scrapers = [
                ReutersScraper, BBCScraper, NYTimesScraper, APNewsScraper,
                BloombergScraper, TheGuardianScraper, CNNScraper, FTScraper,
                SkyNewsScraper, ElPaisScraper, SCMPScraper, AlJazeeraScraper,
                TheVergeScraper
            ]
            
            for S in scrapers:
                try:
                    scraper = S()
                    log.info(f"å¼€å§‹æŠ“å– {scraper.source}...")
                    
                    rows = list(scraper.run(conn))  # ä¼ é€’æ•°æ®åº“è¿æ¥
                    
                    # æ‰¹é‡æ’å…¥æ•°æ®å¹¶ç»Ÿè®¡å®é™…æ’å…¥æ•°
                    inserted_count = 0
                    for it in rows:
                        try:
                            with conn.cursor() as cur:
                                cur.execute(SQL, it.as_dict())
                                if cur.rowcount > 0:  # æ£€æŸ¥æ˜¯å¦çœŸçš„æ’å…¥äº†
                                    inserted_count += 1
                        except Exception as e:
                            if "duplicate key value violates unique constraint" in str(e):
                                log.debug(f"è·³è¿‡é‡å¤URL: {it.url}")
                                continue
                            else:
                                log.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
                                continue
                    
                    total += inserted_count
                    success_count += 1
                    log.info("âœ… %s æŠ“å– %s æ¡ï¼Œæ’å…¥ %s æ¡", scraper.source, len(rows), inserted_count)
                    
                except Exception as e:
                    error_count += 1
                    log.error("âŒ %s æŠ“å–å¤±è´¥: %s", S.__name__, e)
                    
                    # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    import traceback
                    log.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæŠ“å–å™¨ï¼Œä¸ä¸­æ–­æ•´ä¸ªä»»åŠ¡
                    continue
            
            # æ€»ç»“æŠ¥å‘Š
            log.info("ğŸ‰ æŠ“å–å®Œæˆæ€»ç»“:")
            log.info(f"  - æˆåŠŸæŠ“å–: {success_count}/{len(scrapers)} ä¸ªæº")
            log.info(f"  - å¤±è´¥æŠ“å–: {error_count}/{len(scrapers)} ä¸ªæº")
            log.info(f"  - æ€»æ’å…¥è¡Œæ•°: {total} è¡Œ")
            
            return f"inserted {total} rows from {success_count}/{len(scrapers)} sources"
    
    except psycopg2.Error as e:
        log.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise
    except Exception as e:
        log.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        raise

# Airflow DAG é…ç½®
# æ³¨æ„ï¼šAirflow é»˜è®¤ UTCï¼Œéœ€æ¢ç®—åŒ—äº¬æ—¶é—´
# åŒ—äº¬æ—¶é—´ 10:00/17:00/22:00 = UTC 02:00/09:00/14:00
schedule_cron = "0 2,9,14 * * *"  # UTCæ—¶é—´ï¼Œå¯¹åº”åŒ—äº¬æ—¶é—´ 10:00/17:00/22:00

default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "depends_on_past": False,
    "email_on_failure": False,
}

with DAG(
    dag_id="fetch_eventregistry_news",
    description="å¤šæºæ–°é—»ï¼ˆReutersã€BBCã€NYTimesç­‰13ä¸ªæºï¼‰â†’ raw_eventsï¼Œæ¯å¤© 3 æ¬¡ï¼ˆåŒ—äº¬æ—¶é—´ 10:00/17:00/22:00ï¼‰",
    start_date=datetime(2025, 7, 1, tzinfo=pytz.UTC),
    schedule=schedule_cron,
    catchup=False,
    default_args=default_args,
    tags=["news", "eventregistry", "reuters", "bbc", "nytimes", "apnews", "bloomberg", "theguardian", "cnn", "ft", "skynews", "elpais", "scmp", "aljazeera", "theverge"],
) as dag:
    PythonOperator(
        task_id="fetch_eventregistry_news",
        python_callable=_fetch_eventregistry_news,
    ) 