# æ–‡ä»¶è·¯å¾„: pipeline/json_report_generator.py
import logging
from datetime import datetime, timezone
import json
import pandas as pd
import os
import psycopg2
from psycopg2.extras import RealDictCursor

# å¼•å…¥ç»Ÿä¸€çš„æ—¶é—´å·¥å…·æ¨¡å—
from dags.phoenix.time_utils import (
    get_cutoff_hour, logical_date_bj, prev_logical_date_str, utc_to_bj
)

log = logging.getLogger(__name__)

def generate_summary_report_to_json_file(**context):
    """
    ç”Ÿæˆæ¯æ—¥æ–°é—»æ‘˜è¦JSONæŠ¥å‘Š
    ä¸¥æ ¼éµå¾ª"åŒ—äº¬æ—¶é—´6AM"è§„åˆ™è¿›è¡Œæ–°é—»ç­›é€‰
    """
    log.info("ğŸ“Š å¼€å§‹ç”Ÿæˆæ¯æ—¥æ–°é—»æ‘˜è¦JSONæŠ¥å‘Š...")
    
    # è·å–å¯é…ç½®çš„cutoffå°æ—¶æ•°
    try:
        from airflow.models import Variable
        cutoff_hour = get_cutoff_hour(lambda k: Variable.get(k, default_var=None))
    except Exception:
        cutoff_hour = get_cutoff_hour()
    
    log.debug(f"ä½¿ç”¨åŒ—äº¬æ—¶é—´æ—¥ç•Œcutoff={cutoff_hour}å°æ—¶")
    
    # ä»Airflowä¸Šä¸‹æ–‡è·å–DAGçš„é€»è¾‘è¿è¡Œæ—¥æœŸ
    logical_date = context['ds']
    log.info(f"ä¸ºé€»è¾‘æ—¥æœŸ {logical_date} ç”ŸæˆæŠ¥å‘Š...")
    
    # ç›´æ¥ä½¿ç”¨psycopg2è¿æ¥Phoenix V2æ•°æ®åº“
    db_config = {
        'host': 'postgres-phoenix',
        'port': 5432,
        'database': 'phoenix_db',
        'user': 'phoenix_user',
        'password': 'phoenix_pass',
        'options': '-c timezone=Asia/Shanghai'  # è®¾ç½®è¿æ¥æ—¶åŒº
    }
    
    # 1. ã€æ ¸å¿ƒä¿®å¤ã€‘: å®ç°"åŒ—äº¬æ—¶é—´6AM"è§„åˆ™çš„SQLæŸ¥è¯¢ + äº‹ä»¶å»é‡
    # é€»è¾‘: æ•°æ®åº“ä¸­çš„published_atå·²ç»æ˜¯åŒ—äº¬æ—¶é—´ï¼Œç›´æ¥å‡å»cutoff_hourå°æ—¶ï¼Œ
    # ç„¶åå–å…¶æ—¥æœŸéƒ¨åˆ†ä¸DAGçš„é€»è¾‘è¿è¡Œæ—¥æœŸè¿›è¡Œæ¯”è¾ƒã€‚
    # æ–°å¢: ä½¿ç”¨CTEå’Œçª—å£å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªevent_uriæœ€å¤š2ç¯‡æ–‡ç« 
    sql = f"""
    WITH ranked_articles AS (
        SELECT
            final_score_v2, title, body, source_name, url, event_uri, published_at,
            ROW_NUMBER() OVER(
                PARTITION BY event_uri 
                ORDER BY final_score_v2 DESC
            ) as rn
        FROM public.raw_events
        WHERE
            final_score_v2 IS NOT NULL
            AND (published_at - INTERVAL '{cutoff_hour} hours')::date = '{logical_date}'::date
    )
    SELECT
        final_score_v2, title, body, source_name, url, event_uri, published_at
    FROM ranked_articles
    WHERE rn <= 2
    ORDER BY final_score_v2 DESC;
    """
    
    log.info(f"ğŸ“° æ­£åœ¨ä»æ•°æ®åº“ä¸­è¯»å–Top 100å·²æ‰“åˆ†çš„æ–°é—»ï¼ˆåº”ç”¨åŒ—äº¬æ—¶é—´{cutoff_hour}AMæ—¥ç•Œè§„åˆ™ï¼‰...")
    
    try:
        with psycopg2.connect(**db_config) as conn:
            df = pd.read_sql_query(sql, conn)
    except Exception as e:
        log.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        raise
    
    if df.empty:
        log.warning(f"åœ¨ {logical_date} çš„æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°ä»»ä½•å·²æ‰“åˆ†çš„æ–°é—»ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
        return
    
    log.info(f"âœ… æˆåŠŸè¯»å– {len(df)} ç¯‡æ–‡ç« ç”¨äºç”Ÿæˆæ‘˜è¦ã€‚")
    
    # æ·»åŠ debugæ—¥å¿—ï¼Œæ˜¾ç¤ºæ—¶é—´å¤„ç†è¯¦æƒ…
    if df['published_at'].notna().any():
        sample_published_at = df['published_at'].dropna().iloc[0]
        sample_bj = utc_to_bj(sample_published_at)
        sample_logical_date = logical_date_bj(sample_published_at, cutoff_hour)
        log.debug(
            "æ—¶é—´å¤„ç†ç¤ºä¾‹ - cutoff=%s, published_at=%s, bj_time=%s, logical_date=%s",
            cutoff_hour, 
            sample_published_at.isoformat(), 
            sample_bj.isoformat(), 
            sample_logical_date
        )
    
    # 2. æ„å»ºç¬¦åˆ"å¥‘çº¦"çš„JSONå¯¹è±¡
    now_utc = datetime.now(timezone.utc)
    report_data = {
        "reportMetadata": {
            "reportGeneratedAt": now_utc.isoformat(),
            "reportForDate": logical_date,
            "articleCount": len(df),
            "dataSource": "Phoenix V2 Pipeline"
        },
        "articles": []
    }
    
    for index, row in df.iterrows():
        article_object = {
            "rank": index + 1,
            "score": round(row['final_score_v2'], 4) if pd.notna(row['final_score_v2']) else None,
            "source": row['source_name'],
            "title": row['title'],
            "url": row['url'],
            "publishedAt": row['published_at'].isoformat() if pd.notna(row['published_at']) else None,
            "eventId": row['event_uri'],
            "body": row['body']
        }
        report_data["articles"].append(article_object)
    
    # 3. å°†JSONå¯¹è±¡å†™å…¥å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶
    output_dir = "/opt/airflow/exports"
    # æ–‡ä»¶ååŒ…å«é€»è¾‘æ—¥æœŸå’Œæ‰§è¡Œæ—¶é—´ï¼ŒåŒºåˆ†è°ƒåº¦å’Œæ‰‹åŠ¨è¿è¡Œ
    run_type = "scheduled" if context.get('dag_run').run_id.startswith('scheduled') else "manual"
    filename = f"summary_{logical_date}_{run_type}_{now_utc.strftime('%H-%M-%S')}.json"
    output_path = os.path.join(output_dir, filename)
    
    log.info(f"ğŸ’¾ æ­£åœ¨å°†JSONæŠ¥å‘Šå†™å…¥åˆ°æ–‡ä»¶: {output_path}")
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        log.info(f"âœ… JSONæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼æ–‡ä»¶: {filename}")
    except Exception as e:
        log.error(f"âŒ å†™å…¥JSONæŠ¥å‘Šæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise 