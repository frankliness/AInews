"""
æ¯æ—¥æ–°é—»æ±‡æ€»æ¨¡å—
åŸºäºå»åŒè´¨åŒ–å¤„ç†åçš„æ•°æ®ç”Ÿæˆé«˜è´¨é‡æ±‡æ€»æ–‡æ¡£
"""
import pandas as pd
import logging
from typing import List, Dict, Optional
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import os
import json

log = logging.getLogger(__name__)

class DailySummaryGenerator:
    """æ¯æ—¥æ±‡æ€»ç”Ÿæˆå™¨"""
    
    def __init__(self, db_url: str, export_dir: str = "/opt/airflow/exports"):
        """
        åˆå§‹åŒ–æ±‡æ€»ç”Ÿæˆå™¨
        
        Args:
            db_url: æ•°æ®åº“è¿æ¥URL
            export_dir: å¯¼å‡ºç›®å½•
        """
        self.engine = create_engine(db_url)
        self.export_dir = export_dir
        os.makedirs(export_dir, exist_ok=True)
        
    def run(self) -> Dict[str, int]:
        """
        æ‰§è¡Œæ±‡æ€»ä»»åŠ¡
        
        Returns:
            Dict[str, int]: æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        """
        log.info("ğŸš€ å¼€å§‹ç”Ÿæˆæ¯æ—¥æ–°é—»æ±‡æ€»...")
        
        # 1. è·å–å»åŒè´¨åŒ–å¤„ç†åçš„æ•°æ®
        df = self._fetch_deduped_data()
        if df.empty:
            log.warning("æ²¡æœ‰æ‰¾åˆ°å»åŒè´¨åŒ–å¤„ç†åçš„æ•°æ®")
            return {"total_records": 0, "summary_topics": 0}
        
        log.info(f"ğŸ“Š è·å–åˆ° {len(df)} æ¡å»åŒè´¨åŒ–è®°å½•")
        
        # 2. æŒ‰ä¸»é¢˜åˆ†ç»„ï¼Œé€‰æ‹©æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨æ€§æ–°é—»
        summary_data = self._generate_summary_by_topics(df)
        
        # 3. ç”Ÿæˆæ±‡æ€»æ–‡æ¡£
        stats = self._generate_summary_document(summary_data)
        
        log.info(f"âœ… æ±‡æ€»å®Œæˆ: {stats}")
        return stats
    
    def _fetch_deduped_data(self) -> pd.DataFrame:
        """è·å–å»åŒè´¨åŒ–å¤„ç†åçš„æ•°æ®"""
        query = """
            SELECT 
                id, title, body, url, source, published_at,
                topic_id, cluster_size, centroid_sim, score,
                event_id, sentiment
            FROM raw_events
            WHERE published_at >= NOW() - INTERVAL '24 HOURS'
              AND topic_id IS NOT NULL
              AND score IS NOT NULL
            ORDER BY topic_id, score DESC, centroid_sim DESC
        """
        
        try:
            df = pd.read_sql(query, self.engine)
            return df
        except Exception as e:
            log.error(f"è·å–æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _generate_summary_by_topics(self, df: pd.DataFrame) -> List[Dict]:
        """æŒ‰ä¸»é¢˜åˆ†ç»„ï¼Œé€‰æ‹©ä»£è¡¨æ€§æ–°é—»"""
        log.info("ğŸ” æŒ‰ä¸»é¢˜ç”Ÿæˆæ±‡æ€»...")
        
        summary_data = []
        
        for topic_id in df['topic_id'].unique():
            topic_news = df[df['topic_id'] == topic_id].copy()
            
            # é€‰æ‹©è¯¥ä¸»é¢˜ä¸‹åˆ†æ•°æœ€é«˜çš„æ–°é—»ä½œä¸ºä»£è¡¨
            best_news = topic_news.iloc[0]
            
            # æ”¶é›†è¯¥ä¸»é¢˜ä¸‹çš„æ‰€æœ‰æ–°é—»
            all_news = topic_news.head(5).to_dict('records')  # å–å‰5æ¡
            
            # è®¡ç®—ä¸»é¢˜å¹³å‡æƒ…æ„Ÿ
            mode_val = topic_news['sentiment'].mode() if 'sentiment' in topic_news.columns else []
            avg_sentiment = mode_val.iloc[0] if hasattr(mode_val, 'iloc') and len(mode_val) > 0 else 'neutral'
            
            
            # æ™ºèƒ½æˆªæ–­æ–°é—»å†…å®¹
            content = self._smart_truncate_content(best_news['body'])
            
            summary_data.append({
                'topic_id': topic_id,
                'cluster_size': best_news['cluster_size'],
                'representative_news': {
                    'title': best_news['title'],
                    'content': content,
                    'url': best_news['url'],
                    'source': best_news['source'],
                    'score': best_news['score'],
                    'sentiment': 'æœªçŸ¥' if best_news.get('sentiment') is None or (isinstance(best_news.get('sentiment'), float) and best_news.get('sentiment') != best_news.get('sentiment')) else best_news.get('sentiment', 'neutral')
                },
                'related_news': all_news,
                'event_id': best_news.get('event_id', ''),
                'avg_score': topic_news['score'].mean(),
                'avg_sentiment': avg_sentiment
            })
        
        # æŒ‰å¹³å‡åˆ†æ•°æ’åº
        summary_data.sort(key=lambda x: x['avg_score'], reverse=True)
        
        log.info(f"ğŸ“ˆ ç”Ÿæˆäº† {len(summary_data)} ä¸ªä¸»é¢˜çš„æ±‡æ€»")
        return summary_data
    
    def _smart_truncate_content(self, content: str, max_length: int = 800) -> str:
        """
        æ™ºèƒ½æˆªæ–­æ–°é—»å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            max_length: æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            æˆªæ–­åçš„å†…å®¹
        """
        if not content:
            return ""
        
        # å¦‚æœå†…å®¹é•¿åº¦åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
        if len(content) <= max_length:
            return content
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        truncated = content[:max_length]
        
        # æŸ¥æ‰¾æœ€åä¸€ä¸ªå¥å·ã€é—®å·æˆ–æ„Ÿå¹å·
        sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']
        last_sentence_end = -1
        
        for ending in sentence_endings:
            pos = truncated.rfind(ending)
            if pos > last_sentence_end:
                last_sentence_end = pos
        
        # å¦‚æœæ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œåœ¨å¥å­è¾¹ç•Œæˆªæ–­
        if last_sentence_end > max_length * 0.7:  # ç¡®ä¿æˆªæ–­ç‚¹ä¸è¦å¤ªé å‰
            truncated = truncated[:last_sentence_end + 1]
        else:
            # å¦åˆ™åœ¨å•è¯è¾¹ç•Œæˆªæ–­
            last_space = truncated.rfind(' ')
            if last_space > max_length * 0.8:
                truncated = truncated[:last_space]
        
        return truncated + '...'
    
    def _generate_summary_document(self, summary_data: List[Dict]) -> Dict[str, int]:
        """ç”Ÿæˆæ±‡æ€»æ–‡æ¡£"""
        log.info("ğŸ“ ç”Ÿæˆæ±‡æ€»æ–‡æ¡£...")
        
        today = datetime.now().strftime("%Y%m%d")
        timestamp = datetime.now().strftime("%H%M%S")
        filename = f"summary_{today}_{timestamp}.log"
        filepath = os.path.join(self.export_dir, filename)
        
        # ç”Ÿæˆæ±‡æ€»å†…å®¹
        content = self._format_summary_content(summary_data, today)
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # åŒæ—¶ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_filename = f"summary_{today}_{timestamp}.json"
        json_filepath = os.path.join(self.export_dir, json_filename)
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2, default=str)
        
        stats = {
            "total_records": sum(item['cluster_size'] for item in summary_data),
            "summary_topics": len(summary_data),
            "log_file": filename,
            "json_file": json_filename
        }
        
        log.info(f"ğŸ“„ æ±‡æ€»æ–‡æ¡£å·²ç”Ÿæˆ: {filename}")
        return stats
    
    def _format_summary_content(self, summary_data: List[Dict], date: str) -> str:
        """æ ¼å¼åŒ–æ±‡æ€»å†…å®¹"""
        content = f"""# æ—¶æ”¿è§†é¢‘è´¦å·æ¯æ—¥æ–°é—»æ±‡æ€» - {date}

## æ¦‚è¿°
- æ±‡æ€»æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- ä¸»é¢˜æ•°é‡: {len(summary_data)}
- æ€»æ–°é—»æ•°: {sum(item['cluster_size'] for item in summary_data)}

## çƒ­é—¨ä¸»é¢˜æ±‡æ€»

"""
        
        for i, topic in enumerate(summary_data, 1):
            rep_news = topic['representative_news']
            
            content += f"""### {i}. {rep_news['title']}
- **æ¥æº**: {rep_news['source']}
- **ä¸»é¢˜ID**: {topic['topic_id']}
- **èšç±»å¤§å°**: {topic['cluster_size']} æ¡ç›¸å…³æ–°é—»
- **ä»£è¡¨æ€§åˆ†æ•°**: {rep_news['score']:.2f}
- **å¹³å‡åˆ†æ•°**: {topic['avg_score']:.2f}
- **æƒ…æ„Ÿå€¾å‘**: {rep_news['sentiment']}
- **äº‹ä»¶ID**: {topic['event_id']}

**ä»£è¡¨æ€§æ–°é—»æ‘˜è¦**:
{rep_news['content']}

**ç›¸å…³æ–°é—»**:
"""
            
            # æ·»åŠ ç›¸å…³æ–°é—»ï¼Œå¸¦URL
            for j, news in enumerate(topic['related_news'][:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
                url_part = f" - [åŸæ–‡é“¾æ¥]({news['url']})" if news.get('url') else ""
                content += f"{j}. {news['title']} ({news['source']}) - åˆ†æ•°: {news['score']:.2f}{url_part}\n"
            
            content += "\n" + "="*80 + "\n\n"
        
        return content

def run():
    """ä¸»å…¥å£å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“é…ç½®
    db_url = os.getenv('DATABASE_URL', 'postgresql://airflow:airflow@postgres:5432/airflow')
    
    generator = DailySummaryGenerator(db_url)
    return generator.run()

if __name__ == "__main__":
    result = run()
    print(f"æ±‡æ€»å®Œæˆ: {result}") 